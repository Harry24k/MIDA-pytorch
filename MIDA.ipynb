{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 7\n",
    "num_epochs = 500\n",
    "dropout_ratio = 0.5\n",
    "\n",
    "data_path = 'data/BostonHousing.csv'\n",
    "mechanism = 'mcar'\n",
    "method = 'uniform'\n",
    "\n",
    "test_size = 0.3\n",
    "use_cuda = True\n",
    "batch_size  = 1 # not in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path).values\n",
    "\n",
    "rows, cols = data.shape\n",
    "shuffled_index = np.random.permutation(rows)\n",
    "train_index = shuffled_index[:int(rows*(1-test_size))]\n",
    "test_index = shuffled_index[int(rows*(1-test_size)):]\n",
    "\n",
    "train_data = data[train_index, :]\n",
    "test_data = data[test_index, :]\n",
    "\n",
    "# standardized between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_method(raw_data, mechanism='mcar', method='uniform') :\n",
    "    \n",
    "    data = raw_data.copy()\n",
    "    rows, cols = data.shape\n",
    "    \n",
    "    # missingness threshold\n",
    "    t = 0.2\n",
    "    \n",
    "    if mechanism == 'mcar' :\n",
    "    \n",
    "        if method == 'uniform' :\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where v<=t\n",
    "            mask = (v<=t)\n",
    "            data[mask] = 0\n",
    "\n",
    "        elif method == 'random' :\n",
    "            # only half of the attributes to have missing value\n",
    "            missing_cols = np.random.choice(cols, cols//2)\n",
    "            c = np.zeros(cols, dtype=bool)\n",
    "            c[missing_cols] = True\n",
    "\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where v<=t\n",
    "            mask = (v<=t)*c\n",
    "            data[mask] = 0\n",
    "\n",
    "        else :\n",
    "            print(\"Error : There are no such method\")\n",
    "            raise\n",
    "    \n",
    "    elif mechanism == 'mnar' :\n",
    "        \n",
    "        if method == 'uniform' :\n",
    "            # randomly sample two attributes\n",
    "            sample_cols = np.random.choice(cols, 2)\n",
    "\n",
    "            # calculate ther median m1, m2\n",
    "            m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "            m1 = data[:,sample_cols[0]] <= m1\n",
    "            m2 = data[:,sample_cols[1]] >= m2\n",
    "            m = (m1*m2)[:, np.newaxis]\n",
    "\n",
    "            mask = m*(v<=t)\n",
    "            data[mask] = 0\n",
    "\n",
    "\n",
    "        elif method == 'random' :\n",
    "            # only half of the attributes to have missing value\n",
    "            missing_cols = np.random.choice(cols, cols//2)\n",
    "            c = np.zeros(cols, dtype=bool)\n",
    "            c[missing_cols] = True\n",
    "\n",
    "            # randomly sample two attributes\n",
    "            sample_cols = np.random.choice(cols, 2)\n",
    "\n",
    "            # calculate ther median m1, m2\n",
    "            m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "            m1 = data[:,sample_cols[0]] <= m1\n",
    "            m2 = data[:,sample_cols[1]] >= m2\n",
    "            m = (m1*m2)[:, np.newaxis]\n",
    "\n",
    "            mask = m*(v<=t)*c\n",
    "            data[mask] = 0\n",
    "\n",
    "        else :\n",
    "            print(\"Error : There is no such method\")\n",
    "            raise\n",
    "    \n",
    "    else :\n",
    "        print(\"Error : There is no such mechanism\")\n",
    "        raise\n",
    "        \n",
    "    return data, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_data, mask = missing_method(test_data, mechanism=mechanism, method=method)\n",
    "\n",
    "missed_data = torch.from_numpy(missed_data).float()\n",
    "train_data = torch.from_numpy(train_data).float()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.drop_out = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(dim+theta*0, dim+theta*1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*1, dim+theta*2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*2, dim+theta*3)\n",
    "        )\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim+theta*3, dim+theta*2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*2, dim+theta*1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*1, dim+theta*0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.dim)\n",
    "        x_missed = self.drop_out(x)\n",
    "        \n",
    "        z = self.encoder(x_missed)\n",
    "        out = self.decoder(z)\n",
    "        \n",
    "        out = out.view(-1, self.dim)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(dim=cols).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), momentum=0.99, lr=0.01, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], lter [177/354], Loss: 0.064384\n",
      "Epoch [1/500], lter [354/354], Loss: 0.056846\n",
      "Epoch [2/500], lter [177/354], Loss: 0.054899\n",
      "Epoch [2/500], lter [354/354], Loss: 0.033222\n",
      "Epoch [3/500], lter [177/354], Loss: 0.021640\n",
      "Epoch [3/500], lter [354/354], Loss: 0.020006\n",
      "Epoch [4/500], lter [177/354], Loss: 0.139684\n",
      "Epoch [4/500], lter [354/354], Loss: 0.031991\n",
      "Epoch [5/500], lter [177/354], Loss: 0.044147\n",
      "Epoch [5/500], lter [354/354], Loss: 0.078869\n",
      "Epoch [6/500], lter [177/354], Loss: 0.022894\n",
      "Epoch [6/500], lter [354/354], Loss: 0.074245\n",
      "Epoch [7/500], lter [177/354], Loss: 0.167476\n",
      "Epoch [7/500], lter [354/354], Loss: 0.057272\n",
      "Epoch [8/500], lter [177/354], Loss: 0.079521\n",
      "Epoch [8/500], lter [354/354], Loss: 0.050776\n",
      "Epoch [9/500], lter [177/354], Loss: 0.028537\n",
      "Epoch [9/500], lter [354/354], Loss: 0.018806\n",
      "Epoch [10/500], lter [177/354], Loss: 0.068195\n",
      "Epoch [10/500], lter [354/354], Loss: 0.011419\n",
      "Epoch [11/500], lter [177/354], Loss: 0.017908\n",
      "Epoch [11/500], lter [354/354], Loss: 0.015779\n",
      "Epoch [12/500], lter [177/354], Loss: 0.053100\n",
      "Epoch [12/500], lter [354/354], Loss: 0.008185\n",
      "Epoch [13/500], lter [177/354], Loss: 0.007454\n",
      "Epoch [13/500], lter [354/354], Loss: 0.006397\n",
      "Epoch [14/500], lter [177/354], Loss: 0.033548\n",
      "Epoch [14/500], lter [354/354], Loss: 0.017361\n",
      "Epoch [15/500], lter [177/354], Loss: 0.037583\n",
      "Epoch [15/500], lter [354/354], Loss: 0.054357\n",
      "Epoch [16/500], lter [177/354], Loss: 0.081081\n",
      "Epoch [16/500], lter [354/354], Loss: 0.066235\n",
      "Epoch [17/500], lter [177/354], Loss: 0.029273\n",
      "Epoch [17/500], lter [354/354], Loss: 0.032478\n",
      "Epoch [18/500], lter [177/354], Loss: 0.031334\n",
      "Epoch [18/500], lter [354/354], Loss: 0.029699\n",
      "Epoch [19/500], lter [177/354], Loss: 0.015521\n",
      "Epoch [19/500], lter [354/354], Loss: 0.036413\n",
      "Epoch [20/500], lter [177/354], Loss: 0.064395\n",
      "Epoch [20/500], lter [354/354], Loss: 0.092430\n",
      "Epoch [21/500], lter [177/354], Loss: 0.008483\n",
      "Epoch [21/500], lter [354/354], Loss: 0.008506\n",
      "Epoch [22/500], lter [177/354], Loss: 0.086662\n",
      "Epoch [22/500], lter [354/354], Loss: 0.018952\n",
      "Epoch [23/500], lter [177/354], Loss: 0.020633\n",
      "Epoch [23/500], lter [354/354], Loss: 0.008623\n",
      "Epoch [24/500], lter [177/354], Loss: 0.009260\n",
      "Epoch [24/500], lter [354/354], Loss: 0.013292\n",
      "Epoch [25/500], lter [177/354], Loss: 0.021428\n",
      "Epoch [25/500], lter [354/354], Loss: 0.015143\n",
      "Epoch [26/500], lter [177/354], Loss: 0.027416\n",
      "Epoch [26/500], lter [354/354], Loss: 0.010825\n",
      "Epoch [27/500], lter [177/354], Loss: 0.077821\n",
      "Epoch [27/500], lter [354/354], Loss: 0.037511\n",
      "Epoch [28/500], lter [177/354], Loss: 0.020606\n",
      "Epoch [28/500], lter [354/354], Loss: 0.022198\n",
      "Epoch [29/500], lter [177/354], Loss: 0.023742\n",
      "Epoch [29/500], lter [354/354], Loss: 0.014050\n",
      "Epoch [30/500], lter [177/354], Loss: 0.038778\n",
      "Epoch [30/500], lter [354/354], Loss: 0.041259\n",
      "Epoch [31/500], lter [177/354], Loss: 0.022900\n",
      "Epoch [31/500], lter [354/354], Loss: 0.040035\n",
      "Epoch [32/500], lter [177/354], Loss: 0.013150\n",
      "Epoch [32/500], lter [354/354], Loss: 0.008087\n",
      "Epoch [33/500], lter [177/354], Loss: 0.071368\n",
      "Epoch [33/500], lter [354/354], Loss: 0.028056\n",
      "Epoch [34/500], lter [177/354], Loss: 0.031586\n",
      "Epoch [34/500], lter [354/354], Loss: 0.015288\n",
      "Epoch [35/500], lter [177/354], Loss: 0.023557\n",
      "Epoch [35/500], lter [354/354], Loss: 0.045095\n",
      "Epoch [36/500], lter [177/354], Loss: 0.022436\n",
      "Epoch [36/500], lter [354/354], Loss: 0.015073\n",
      "Epoch [37/500], lter [177/354], Loss: 0.007628\n",
      "Epoch [37/500], lter [354/354], Loss: 0.045133\n",
      "Epoch [38/500], lter [177/354], Loss: 0.049518\n",
      "Epoch [38/500], lter [354/354], Loss: 0.007259\n",
      "Epoch [39/500], lter [177/354], Loss: 0.051683\n",
      "Epoch [39/500], lter [354/354], Loss: 0.047775\n",
      "Epoch [40/500], lter [177/354], Loss: 0.043413\n",
      "Epoch [40/500], lter [354/354], Loss: 0.029319\n",
      "Epoch [41/500], lter [177/354], Loss: 0.143694\n",
      "Epoch [41/500], lter [354/354], Loss: 0.011680\n",
      "Epoch [42/500], lter [177/354], Loss: 0.065215\n",
      "Epoch [42/500], lter [354/354], Loss: 0.068090\n",
      "Epoch [43/500], lter [177/354], Loss: 0.019263\n",
      "Epoch [43/500], lter [354/354], Loss: 0.009305\n",
      "Epoch [44/500], lter [177/354], Loss: 0.011848\n",
      "Epoch [44/500], lter [354/354], Loss: 0.023467\n",
      "Epoch [45/500], lter [177/354], Loss: 0.049585\n",
      "Epoch [45/500], lter [354/354], Loss: 0.010518\n",
      "Epoch [46/500], lter [177/354], Loss: 0.042660\n",
      "Epoch [46/500], lter [354/354], Loss: 0.014617\n",
      "Epoch [47/500], lter [177/354], Loss: 0.011457\n",
      "Epoch [47/500], lter [354/354], Loss: 0.008824\n",
      "Epoch [48/500], lter [177/354], Loss: 0.028369\n",
      "Epoch [48/500], lter [354/354], Loss: 0.034598\n",
      "Epoch [49/500], lter [177/354], Loss: 0.036810\n",
      "Epoch [49/500], lter [354/354], Loss: 0.011664\n",
      "Epoch [50/500], lter [177/354], Loss: 0.107978\n",
      "Epoch [50/500], lter [354/354], Loss: 0.013504\n",
      "Epoch [51/500], lter [177/354], Loss: 0.060351\n",
      "Epoch [51/500], lter [354/354], Loss: 0.028521\n",
      "Epoch [52/500], lter [177/354], Loss: 0.018882\n",
      "Epoch [52/500], lter [354/354], Loss: 0.006224\n",
      "Epoch [53/500], lter [177/354], Loss: 0.038295\n",
      "Epoch [53/500], lter [354/354], Loss: 0.107213\n",
      "Epoch [54/500], lter [177/354], Loss: 0.006020\n",
      "Epoch [54/500], lter [354/354], Loss: 0.011832\n",
      "Epoch [55/500], lter [177/354], Loss: 0.035299\n",
      "Epoch [55/500], lter [354/354], Loss: 0.222697\n",
      "Epoch [56/500], lter [177/354], Loss: 0.066219\n",
      "Epoch [56/500], lter [354/354], Loss: 0.099921\n",
      "Epoch [57/500], lter [177/354], Loss: 0.027873\n",
      "Epoch [57/500], lter [354/354], Loss: 0.013966\n",
      "Epoch [58/500], lter [177/354], Loss: 0.006189\n",
      "Epoch [58/500], lter [354/354], Loss: 0.016805\n",
      "Epoch [59/500], lter [177/354], Loss: 0.032970\n",
      "Epoch [59/500], lter [354/354], Loss: 0.019082\n",
      "Epoch [60/500], lter [177/354], Loss: 0.067412\n",
      "Epoch [60/500], lter [354/354], Loss: 0.026900\n",
      "Epoch [61/500], lter [177/354], Loss: 0.016343\n",
      "Epoch [61/500], lter [354/354], Loss: 0.014179\n",
      "Epoch [62/500], lter [177/354], Loss: 0.034891\n",
      "Epoch [62/500], lter [354/354], Loss: 0.056654\n",
      "Epoch [63/500], lter [177/354], Loss: 0.051164\n",
      "Epoch [63/500], lter [354/354], Loss: 0.010034\n",
      "Epoch [64/500], lter [177/354], Loss: 0.078526\n",
      "Epoch [64/500], lter [354/354], Loss: 0.014101\n",
      "Epoch [65/500], lter [177/354], Loss: 0.013137\n",
      "Epoch [65/500], lter [354/354], Loss: 0.015967\n",
      "Epoch [66/500], lter [177/354], Loss: 0.043357\n",
      "Epoch [66/500], lter [354/354], Loss: 0.046046\n",
      "Epoch [67/500], lter [177/354], Loss: 0.017796\n",
      "Epoch [67/500], lter [354/354], Loss: 0.021559\n",
      "Epoch [68/500], lter [177/354], Loss: 0.057097\n",
      "Epoch [68/500], lter [354/354], Loss: 0.015074\n",
      "Epoch [69/500], lter [177/354], Loss: 0.017963\n",
      "Epoch [69/500], lter [354/354], Loss: 0.005301\n",
      "Epoch [70/500], lter [177/354], Loss: 0.028489\n",
      "Epoch [70/500], lter [354/354], Loss: 0.021797\n",
      "Epoch [71/500], lter [177/354], Loss: 0.040458\n",
      "Epoch [71/500], lter [354/354], Loss: 0.012436\n",
      "Epoch [72/500], lter [177/354], Loss: 0.021030\n",
      "Epoch [72/500], lter [354/354], Loss: 0.116195\n",
      "Epoch [73/500], lter [177/354], Loss: 0.038416\n",
      "Epoch [73/500], lter [354/354], Loss: 0.006120\n",
      "Epoch [74/500], lter [177/354], Loss: 0.026059\n",
      "Epoch [74/500], lter [354/354], Loss: 0.018052\n",
      "Epoch [75/500], lter [177/354], Loss: 0.013579\n",
      "Epoch [75/500], lter [354/354], Loss: 0.035927\n",
      "Epoch [76/500], lter [177/354], Loss: 0.013956\n",
      "Epoch [76/500], lter [354/354], Loss: 0.014016\n",
      "Epoch [77/500], lter [177/354], Loss: 0.032773\n",
      "Epoch [77/500], lter [354/354], Loss: 0.019421\n",
      "Epoch [78/500], lter [177/354], Loss: 0.015983\n",
      "Epoch [78/500], lter [354/354], Loss: 0.076261\n",
      "Epoch [79/500], lter [177/354], Loss: 0.013049\n",
      "Epoch [79/500], lter [354/354], Loss: 0.026755\n",
      "Epoch [80/500], lter [177/354], Loss: 0.047658\n",
      "Epoch [80/500], lter [354/354], Loss: 0.013988\n",
      "Epoch [81/500], lter [177/354], Loss: 0.004482\n",
      "Epoch [81/500], lter [354/354], Loss: 0.018193\n",
      "Epoch [82/500], lter [177/354], Loss: 0.026571\n",
      "Epoch [82/500], lter [354/354], Loss: 0.025752\n",
      "Epoch [83/500], lter [177/354], Loss: 0.028137\n",
      "Epoch [83/500], lter [354/354], Loss: 0.062716\n",
      "Epoch [84/500], lter [177/354], Loss: 0.040382\n",
      "Epoch [84/500], lter [354/354], Loss: 0.012786\n",
      "Epoch [85/500], lter [177/354], Loss: 0.012533\n",
      "Epoch [85/500], lter [354/354], Loss: 0.027144\n",
      "Epoch [86/500], lter [177/354], Loss: 0.130064\n",
      "Epoch [86/500], lter [354/354], Loss: 0.020099\n",
      "Epoch [87/500], lter [177/354], Loss: 0.035107\n",
      "Epoch [87/500], lter [354/354], Loss: 0.005317\n",
      "Epoch [88/500], lter [177/354], Loss: 0.024417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/500], lter [354/354], Loss: 0.016583\n",
      "Epoch [89/500], lter [177/354], Loss: 0.096540\n",
      "Epoch [89/500], lter [354/354], Loss: 0.012086\n",
      "Epoch [90/500], lter [177/354], Loss: 0.013446\n",
      "Epoch [90/500], lter [354/354], Loss: 0.015077\n",
      "Epoch [91/500], lter [177/354], Loss: 0.040354\n",
      "Epoch [91/500], lter [354/354], Loss: 0.013960\n",
      "Epoch [92/500], lter [177/354], Loss: 0.077843\n",
      "Epoch [92/500], lter [354/354], Loss: 0.072892\n",
      "Epoch [93/500], lter [177/354], Loss: 0.086571\n",
      "Epoch [93/500], lter [354/354], Loss: 0.032262\n",
      "Epoch [94/500], lter [177/354], Loss: 0.009919\n",
      "Epoch [94/500], lter [354/354], Loss: 0.051995\n",
      "Epoch [95/500], lter [177/354], Loss: 0.024357\n",
      "Epoch [95/500], lter [354/354], Loss: 0.019331\n",
      "Epoch [96/500], lter [177/354], Loss: 0.016462\n",
      "Epoch [96/500], lter [354/354], Loss: 0.008812\n",
      "Epoch [97/500], lter [177/354], Loss: 0.010586\n",
      "Epoch [97/500], lter [354/354], Loss: 0.085507\n",
      "Epoch [98/500], lter [177/354], Loss: 0.008209\n",
      "Epoch [98/500], lter [354/354], Loss: 0.084329\n",
      "Epoch [99/500], lter [177/354], Loss: 0.030688\n",
      "Epoch [99/500], lter [354/354], Loss: 0.024030\n",
      "Epoch [100/500], lter [177/354], Loss: 0.017686\n",
      "Epoch [100/500], lter [354/354], Loss: 0.020231\n",
      "Epoch [101/500], lter [177/354], Loss: 0.009746\n",
      "Epoch [101/500], lter [354/354], Loss: 0.036693\n",
      "Epoch [102/500], lter [177/354], Loss: 0.031722\n",
      "Epoch [102/500], lter [354/354], Loss: 0.019563\n",
      "Epoch [103/500], lter [177/354], Loss: 0.027846\n",
      "Epoch [103/500], lter [354/354], Loss: 0.023253\n",
      "Epoch [104/500], lter [177/354], Loss: 0.039855\n",
      "Epoch [104/500], lter [354/354], Loss: 0.007659\n",
      "Epoch [105/500], lter [177/354], Loss: 0.023778\n",
      "Epoch [105/500], lter [354/354], Loss: 0.020562\n",
      "Epoch [106/500], lter [177/354], Loss: 0.028023\n",
      "Epoch [106/500], lter [354/354], Loss: 0.021738\n",
      "Epoch [107/500], lter [177/354], Loss: 0.007256\n",
      "Epoch [107/500], lter [354/354], Loss: 0.013230\n",
      "Epoch [108/500], lter [177/354], Loss: 0.071364\n",
      "Epoch [108/500], lter [354/354], Loss: 0.021043\n",
      "Epoch [109/500], lter [177/354], Loss: 0.010096\n",
      "Epoch [109/500], lter [354/354], Loss: 0.081748\n",
      "Epoch [110/500], lter [177/354], Loss: 0.009121\n",
      "Epoch [110/500], lter [354/354], Loss: 0.042105\n",
      "Epoch [111/500], lter [177/354], Loss: 0.010004\n",
      "Epoch [111/500], lter [354/354], Loss: 0.017931\n",
      "Epoch [112/500], lter [177/354], Loss: 0.071318\n",
      "Epoch [112/500], lter [354/354], Loss: 0.006881\n",
      "Epoch [113/500], lter [177/354], Loss: 0.021469\n",
      "Epoch [113/500], lter [354/354], Loss: 0.037019\n",
      "Epoch [114/500], lter [177/354], Loss: 0.025362\n",
      "Epoch [114/500], lter [354/354], Loss: 0.065843\n",
      "Epoch [115/500], lter [177/354], Loss: 0.067482\n",
      "Epoch [115/500], lter [354/354], Loss: 0.020079\n",
      "Epoch [116/500], lter [177/354], Loss: 0.026035\n",
      "Epoch [116/500], lter [354/354], Loss: 0.106947\n",
      "Epoch [117/500], lter [177/354], Loss: 0.088989\n",
      "Epoch [117/500], lter [354/354], Loss: 0.061043\n",
      "Epoch [118/500], lter [177/354], Loss: 0.010709\n",
      "Epoch [118/500], lter [354/354], Loss: 0.014830\n",
      "Epoch [119/500], lter [177/354], Loss: 0.052142\n",
      "Epoch [119/500], lter [354/354], Loss: 0.006042\n",
      "Epoch [120/500], lter [177/354], Loss: 0.025174\n",
      "Epoch [120/500], lter [354/354], Loss: 0.016661\n",
      "Epoch [121/500], lter [177/354], Loss: 0.007875\n",
      "Epoch [121/500], lter [354/354], Loss: 0.046654\n",
      "Epoch [122/500], lter [177/354], Loss: 0.048096\n",
      "Epoch [122/500], lter [354/354], Loss: 0.008011\n",
      "Epoch [123/500], lter [177/354], Loss: 0.035473\n",
      "Epoch [123/500], lter [354/354], Loss: 0.022117\n",
      "Epoch [124/500], lter [177/354], Loss: 0.044256\n",
      "Epoch [124/500], lter [354/354], Loss: 0.022325\n",
      "Epoch [125/500], lter [177/354], Loss: 0.032785\n",
      "Epoch [125/500], lter [354/354], Loss: 0.009723\n",
      "Epoch [126/500], lter [177/354], Loss: 0.035685\n",
      "Epoch [126/500], lter [354/354], Loss: 0.032091\n",
      "Epoch [127/500], lter [177/354], Loss: 0.013392\n",
      "Epoch [127/500], lter [354/354], Loss: 0.058350\n",
      "Epoch [128/500], lter [177/354], Loss: 0.104721\n",
      "Epoch [128/500], lter [354/354], Loss: 0.071236\n",
      "Epoch [129/500], lter [177/354], Loss: 0.036293\n",
      "Epoch [129/500], lter [354/354], Loss: 0.051234\n",
      "Epoch [130/500], lter [177/354], Loss: 0.006252\n",
      "Epoch [130/500], lter [354/354], Loss: 0.045191\n",
      "Epoch [131/500], lter [177/354], Loss: 0.013488\n",
      "Epoch [131/500], lter [354/354], Loss: 0.005925\n",
      "Epoch [132/500], lter [177/354], Loss: 0.008035\n",
      "Epoch [132/500], lter [354/354], Loss: 0.029931\n",
      "Epoch [133/500], lter [177/354], Loss: 0.019873\n",
      "Epoch [133/500], lter [354/354], Loss: 0.017360\n",
      "Epoch [134/500], lter [177/354], Loss: 0.016860\n",
      "Epoch [134/500], lter [354/354], Loss: 0.018519\n",
      "Epoch [135/500], lter [177/354], Loss: 0.017339\n",
      "Epoch [135/500], lter [354/354], Loss: 0.106117\n",
      "Epoch [136/500], lter [177/354], Loss: 0.016584\n",
      "Epoch [136/500], lter [354/354], Loss: 0.039208\n",
      "Epoch [137/500], lter [177/354], Loss: 0.010591\n",
      "Epoch [137/500], lter [354/354], Loss: 0.009873\n",
      "Epoch [138/500], lter [177/354], Loss: 0.005562\n",
      "Epoch [138/500], lter [354/354], Loss: 0.027775\n",
      "Epoch [139/500], lter [177/354], Loss: 0.019883\n",
      "Epoch [139/500], lter [354/354], Loss: 0.042107\n",
      "Epoch [140/500], lter [177/354], Loss: 0.070649\n",
      "Epoch [140/500], lter [354/354], Loss: 0.022221\n",
      "Epoch [141/500], lter [177/354], Loss: 0.035592\n",
      "Epoch [141/500], lter [354/354], Loss: 0.012727\n",
      "Epoch [142/500], lter [177/354], Loss: 0.062240\n",
      "Epoch [142/500], lter [354/354], Loss: 0.034417\n",
      "Epoch [143/500], lter [177/354], Loss: 0.031843\n",
      "Epoch [143/500], lter [354/354], Loss: 0.018138\n",
      "Epoch [144/500], lter [177/354], Loss: 0.021571\n",
      "Epoch [144/500], lter [354/354], Loss: 0.014747\n",
      "Epoch [145/500], lter [177/354], Loss: 0.039113\n",
      "Epoch [145/500], lter [354/354], Loss: 0.009270\n",
      "Epoch [146/500], lter [177/354], Loss: 0.020615\n",
      "Epoch [146/500], lter [354/354], Loss: 0.010954\n",
      "Epoch [147/500], lter [177/354], Loss: 0.018947\n",
      "Epoch [147/500], lter [354/354], Loss: 0.026490\n",
      "Epoch [148/500], lter [177/354], Loss: 0.030442\n",
      "Epoch [148/500], lter [354/354], Loss: 0.016763\n",
      "Epoch [149/500], lter [177/354], Loss: 0.090449\n",
      "Epoch [149/500], lter [354/354], Loss: 0.010544\n",
      "Epoch [150/500], lter [177/354], Loss: 0.019149\n",
      "Epoch [150/500], lter [354/354], Loss: 0.051062\n",
      "Epoch [151/500], lter [177/354], Loss: 0.022709\n",
      "Epoch [151/500], lter [354/354], Loss: 0.024004\n",
      "Epoch [152/500], lter [177/354], Loss: 0.021812\n",
      "Epoch [152/500], lter [354/354], Loss: 0.007424\n",
      "Epoch [153/500], lter [177/354], Loss: 0.021730\n",
      "Epoch [153/500], lter [354/354], Loss: 0.027427\n",
      "Epoch [154/500], lter [177/354], Loss: 0.024874\n",
      "Epoch [154/500], lter [354/354], Loss: 0.025546\n",
      "Epoch [155/500], lter [177/354], Loss: 0.018216\n",
      "Epoch [155/500], lter [354/354], Loss: 0.019446\n",
      "Epoch [156/500], lter [177/354], Loss: 0.008239\n",
      "Epoch [156/500], lter [354/354], Loss: 0.051831\n",
      "Epoch [157/500], lter [177/354], Loss: 0.014292\n",
      "Epoch [157/500], lter [354/354], Loss: 0.061426\n",
      "Epoch [158/500], lter [177/354], Loss: 0.029751\n",
      "Epoch [158/500], lter [354/354], Loss: 0.008517\n",
      "Epoch [159/500], lter [177/354], Loss: 0.030711\n",
      "Epoch [159/500], lter [354/354], Loss: 0.056447\n",
      "Epoch [160/500], lter [177/354], Loss: 0.031139\n",
      "Epoch [160/500], lter [354/354], Loss: 0.024090\n",
      "Epoch [161/500], lter [177/354], Loss: 0.008001\n",
      "Epoch [161/500], lter [354/354], Loss: 0.013826\n",
      "Epoch [162/500], lter [177/354], Loss: 0.023390\n",
      "Epoch [162/500], lter [354/354], Loss: 0.035301\n",
      "Epoch [163/500], lter [177/354], Loss: 0.014524\n",
      "Epoch [163/500], lter [354/354], Loss: 0.027679\n",
      "Epoch [164/500], lter [177/354], Loss: 0.018311\n",
      "Epoch [164/500], lter [354/354], Loss: 0.031544\n",
      "Epoch [165/500], lter [177/354], Loss: 0.007907\n",
      "Epoch [165/500], lter [354/354], Loss: 0.078608\n",
      "Epoch [166/500], lter [177/354], Loss: 0.036466\n",
      "Epoch [166/500], lter [354/354], Loss: 0.013794\n",
      "Epoch [167/500], lter [177/354], Loss: 0.014361\n",
      "Epoch [167/500], lter [354/354], Loss: 0.033197\n",
      "Epoch [168/500], lter [177/354], Loss: 0.072228\n",
      "Epoch [168/500], lter [354/354], Loss: 0.013269\n",
      "Epoch [169/500], lter [177/354], Loss: 0.007252\n",
      "Epoch [169/500], lter [354/354], Loss: 0.038044\n",
      "Epoch [170/500], lter [177/354], Loss: 0.004089\n",
      "Epoch [170/500], lter [354/354], Loss: 0.009669\n",
      "Epoch [171/500], lter [177/354], Loss: 0.053817\n",
      "Epoch [171/500], lter [354/354], Loss: 0.025642\n",
      "Epoch [172/500], lter [177/354], Loss: 0.016261\n",
      "Epoch [172/500], lter [354/354], Loss: 0.053792\n",
      "Epoch [173/500], lter [177/354], Loss: 0.016135\n",
      "Epoch [173/500], lter [354/354], Loss: 0.023338\n",
      "Epoch [174/500], lter [177/354], Loss: 0.018111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [174/500], lter [354/354], Loss: 0.018363\n",
      "Epoch [175/500], lter [177/354], Loss: 0.012584\n",
      "Epoch [175/500], lter [354/354], Loss: 0.007947\n",
      "Epoch [176/500], lter [177/354], Loss: 0.022341\n",
      "Epoch [176/500], lter [354/354], Loss: 0.014584\n",
      "Epoch [177/500], lter [177/354], Loss: 0.017245\n",
      "Epoch [177/500], lter [354/354], Loss: 0.006471\n",
      "Epoch [178/500], lter [177/354], Loss: 0.017742\n",
      "Epoch [178/500], lter [354/354], Loss: 0.068118\n",
      "Epoch [179/500], lter [177/354], Loss: 0.019092\n",
      "Epoch [179/500], lter [354/354], Loss: 0.022993\n",
      "Epoch [180/500], lter [177/354], Loss: 0.013789\n",
      "Epoch [180/500], lter [354/354], Loss: 0.019468\n",
      "Epoch [181/500], lter [177/354], Loss: 0.010613\n",
      "Epoch [181/500], lter [354/354], Loss: 0.007876\n",
      "Epoch [182/500], lter [177/354], Loss: 0.003555\n",
      "Epoch [182/500], lter [354/354], Loss: 0.023834\n",
      "Epoch [183/500], lter [177/354], Loss: 0.032061\n",
      "Epoch [183/500], lter [354/354], Loss: 0.015184\n",
      "Epoch [184/500], lter [177/354], Loss: 0.043588\n",
      "Epoch [184/500], lter [354/354], Loss: 0.028740\n",
      "Epoch [185/500], lter [177/354], Loss: 0.005721\n",
      "Epoch [185/500], lter [354/354], Loss: 0.023841\n",
      "Epoch [186/500], lter [177/354], Loss: 0.030318\n",
      "Epoch [186/500], lter [354/354], Loss: 0.011173\n",
      "Epoch [187/500], lter [177/354], Loss: 0.008891\n",
      "Epoch [187/500], lter [354/354], Loss: 0.004655\n",
      "Epoch [188/500], lter [177/354], Loss: 0.117677\n",
      "Epoch [188/500], lter [354/354], Loss: 0.065317\n",
      "Epoch [189/500], lter [177/354], Loss: 0.057534\n",
      "Epoch [189/500], lter [354/354], Loss: 0.019650\n",
      "Epoch [190/500], lter [177/354], Loss: 0.033663\n",
      "Epoch [190/500], lter [354/354], Loss: 0.017957\n",
      "Epoch [191/500], lter [177/354], Loss: 0.101174\n",
      "Epoch [191/500], lter [354/354], Loss: 0.010410\n",
      "Epoch [192/500], lter [177/354], Loss: 0.022715\n",
      "Epoch [192/500], lter [354/354], Loss: 0.004550\n",
      "Epoch [193/500], lter [177/354], Loss: 0.040885\n",
      "Epoch [193/500], lter [354/354], Loss: 0.016120\n",
      "Epoch [194/500], lter [177/354], Loss: 0.013246\n",
      "Epoch [194/500], lter [354/354], Loss: 0.009937\n",
      "Epoch [195/500], lter [177/354], Loss: 0.029742\n",
      "Epoch [195/500], lter [354/354], Loss: 0.033305\n",
      "Epoch [196/500], lter [177/354], Loss: 0.011106\n",
      "Epoch [196/500], lter [354/354], Loss: 0.009335\n",
      "Epoch [197/500], lter [177/354], Loss: 0.016255\n",
      "Epoch [197/500], lter [354/354], Loss: 0.021485\n",
      "Epoch [198/500], lter [177/354], Loss: 0.014713\n",
      "Epoch [198/500], lter [354/354], Loss: 0.014387\n",
      "Epoch [199/500], lter [177/354], Loss: 0.012271\n",
      "Epoch [199/500], lter [354/354], Loss: 0.022195\n",
      "Epoch [200/500], lter [177/354], Loss: 0.013193\n",
      "Epoch [200/500], lter [354/354], Loss: 0.012934\n",
      "Epoch [201/500], lter [177/354], Loss: 0.021756\n",
      "Epoch [201/500], lter [354/354], Loss: 0.038815\n",
      "Epoch [202/500], lter [177/354], Loss: 0.017557\n",
      "Epoch [202/500], lter [354/354], Loss: 0.016176\n",
      "Epoch [203/500], lter [177/354], Loss: 0.009344\n",
      "Epoch [203/500], lter [354/354], Loss: 0.022741\n",
      "Epoch [204/500], lter [177/354], Loss: 0.022483\n",
      "Epoch [204/500], lter [354/354], Loss: 0.014760\n",
      "Epoch [205/500], lter [177/354], Loss: 0.010284\n",
      "Epoch [205/500], lter [354/354], Loss: 0.032703\n",
      "Epoch [206/500], lter [177/354], Loss: 0.012181\n",
      "Epoch [206/500], lter [354/354], Loss: 0.016629\n",
      "Epoch [207/500], lter [177/354], Loss: 0.012715\n",
      "Epoch [207/500], lter [354/354], Loss: 0.017976\n",
      "Epoch [208/500], lter [177/354], Loss: 0.034718\n",
      "Epoch [208/500], lter [354/354], Loss: 0.010751\n",
      "Epoch [209/500], lter [177/354], Loss: 0.012516\n",
      "Epoch [209/500], lter [354/354], Loss: 0.004155\n",
      "Epoch [210/500], lter [177/354], Loss: 0.016450\n",
      "Epoch [210/500], lter [354/354], Loss: 0.010119\n",
      "Epoch [211/500], lter [177/354], Loss: 0.012466\n",
      "Epoch [211/500], lter [354/354], Loss: 0.013734\n",
      "Epoch [212/500], lter [177/354], Loss: 0.019295\n",
      "Epoch [212/500], lter [354/354], Loss: 0.052274\n",
      "Epoch [213/500], lter [177/354], Loss: 0.016651\n",
      "Epoch [213/500], lter [354/354], Loss: 0.018828\n",
      "Epoch [214/500], lter [177/354], Loss: 0.016807\n",
      "Epoch [214/500], lter [354/354], Loss: 0.015170\n",
      "Epoch [215/500], lter [177/354], Loss: 0.013505\n",
      "Epoch [215/500], lter [354/354], Loss: 0.023833\n",
      "Epoch [216/500], lter [177/354], Loss: 0.017870\n",
      "Epoch [216/500], lter [354/354], Loss: 0.008638\n",
      "Epoch [217/500], lter [177/354], Loss: 0.015375\n",
      "Epoch [217/500], lter [354/354], Loss: 0.039630\n",
      "Epoch [218/500], lter [177/354], Loss: 0.042622\n",
      "Epoch [218/500], lter [354/354], Loss: 0.005845\n",
      "Epoch [219/500], lter [177/354], Loss: 0.040240\n",
      "Epoch [219/500], lter [354/354], Loss: 0.019664\n",
      "Epoch [220/500], lter [177/354], Loss: 0.023428\n",
      "Epoch [220/500], lter [354/354], Loss: 0.029499\n",
      "Epoch [221/500], lter [177/354], Loss: 0.089135\n",
      "Epoch [221/500], lter [354/354], Loss: 0.136368\n",
      "Epoch [222/500], lter [177/354], Loss: 0.027485\n",
      "Epoch [222/500], lter [354/354], Loss: 0.133224\n",
      "Epoch [223/500], lter [177/354], Loss: 0.018986\n",
      "Epoch [223/500], lter [354/354], Loss: 0.068244\n",
      "Epoch [224/500], lter [177/354], Loss: 0.011312\n",
      "Epoch [224/500], lter [354/354], Loss: 0.018946\n",
      "Epoch [225/500], lter [177/354], Loss: 0.057268\n",
      "Epoch [225/500], lter [354/354], Loss: 0.010923\n",
      "Epoch [226/500], lter [177/354], Loss: 0.008062\n",
      "Epoch [226/500], lter [354/354], Loss: 0.005426\n",
      "Epoch [227/500], lter [177/354], Loss: 0.014663\n",
      "Epoch [227/500], lter [354/354], Loss: 0.007345\n",
      "Epoch [228/500], lter [177/354], Loss: 0.106815\n",
      "Epoch [228/500], lter [354/354], Loss: 0.079184\n",
      "Epoch [229/500], lter [177/354], Loss: 0.026862\n",
      "Epoch [229/500], lter [354/354], Loss: 0.007207\n",
      "Epoch [230/500], lter [177/354], Loss: 0.105815\n",
      "Epoch [230/500], lter [354/354], Loss: 0.010420\n",
      "Epoch [231/500], lter [177/354], Loss: 0.005001\n",
      "Epoch [231/500], lter [354/354], Loss: 0.025265\n",
      "Epoch [232/500], lter [177/354], Loss: 0.019721\n",
      "Epoch [232/500], lter [354/354], Loss: 0.013792\n",
      "Epoch [233/500], lter [177/354], Loss: 0.009588\n",
      "Epoch [233/500], lter [354/354], Loss: 0.011774\n",
      "Epoch [234/500], lter [177/354], Loss: 0.014266\n",
      "Epoch [234/500], lter [354/354], Loss: 0.012243\n",
      "Epoch [235/500], lter [177/354], Loss: 0.008376\n",
      "Epoch [235/500], lter [354/354], Loss: 0.029733\n",
      "Epoch [236/500], lter [177/354], Loss: 0.023274\n",
      "Epoch [236/500], lter [354/354], Loss: 0.024971\n",
      "Epoch [237/500], lter [177/354], Loss: 0.009389\n",
      "Epoch [237/500], lter [354/354], Loss: 0.040002\n",
      "Epoch [238/500], lter [177/354], Loss: 0.029283\n",
      "Epoch [238/500], lter [354/354], Loss: 0.035366\n",
      "Epoch [239/500], lter [177/354], Loss: 0.010558\n",
      "Epoch [239/500], lter [354/354], Loss: 0.012354\n",
      "Epoch [240/500], lter [177/354], Loss: 0.043637\n",
      "Epoch [240/500], lter [354/354], Loss: 0.009579\n",
      "Epoch [241/500], lter [177/354], Loss: 0.024250\n",
      "Epoch [241/500], lter [354/354], Loss: 0.009495\n",
      "Epoch [242/500], lter [177/354], Loss: 0.070462\n",
      "Epoch [242/500], lter [354/354], Loss: 0.030274\n",
      "Epoch [243/500], lter [177/354], Loss: 0.016244\n",
      "Epoch [243/500], lter [354/354], Loss: 0.047420\n",
      "Epoch [244/500], lter [177/354], Loss: 0.021121\n",
      "Epoch [244/500], lter [354/354], Loss: 0.004325\n",
      "Epoch [245/500], lter [177/354], Loss: 0.037521\n",
      "Epoch [245/500], lter [354/354], Loss: 0.021242\n",
      "Epoch [246/500], lter [177/354], Loss: 0.011736\n",
      "Epoch [246/500], lter [354/354], Loss: 0.014424\n",
      "Epoch [247/500], lter [177/354], Loss: 0.010503\n",
      "Epoch [247/500], lter [354/354], Loss: 0.025217\n",
      "Epoch [248/500], lter [177/354], Loss: 0.044571\n",
      "Epoch [248/500], lter [354/354], Loss: 0.034685\n",
      "Epoch [249/500], lter [177/354], Loss: 0.009288\n",
      "Epoch [249/500], lter [354/354], Loss: 0.020159\n",
      "Epoch [250/500], lter [177/354], Loss: 0.020524\n",
      "Epoch [250/500], lter [354/354], Loss: 0.095451\n",
      "Epoch [251/500], lter [177/354], Loss: 0.005279\n",
      "Epoch [251/500], lter [354/354], Loss: 0.004151\n",
      "Epoch [252/500], lter [177/354], Loss: 0.049192\n",
      "Epoch [252/500], lter [354/354], Loss: 0.010003\n",
      "Epoch [253/500], lter [177/354], Loss: 0.020191\n",
      "Epoch [253/500], lter [354/354], Loss: 0.006529\n",
      "Epoch [254/500], lter [177/354], Loss: 0.032964\n",
      "Epoch [254/500], lter [354/354], Loss: 0.055121\n",
      "Epoch [255/500], lter [177/354], Loss: 0.033845\n",
      "Epoch [255/500], lter [354/354], Loss: 0.018293\n",
      "Epoch [256/500], lter [177/354], Loss: 0.023784\n",
      "Epoch [256/500], lter [354/354], Loss: 0.011209\n",
      "Epoch [257/500], lter [177/354], Loss: 0.014394\n",
      "Epoch [257/500], lter [354/354], Loss: 0.024305\n",
      "Epoch [258/500], lter [177/354], Loss: 0.020293\n",
      "Epoch [258/500], lter [354/354], Loss: 0.062381\n",
      "Epoch [259/500], lter [177/354], Loss: 0.023867\n",
      "Epoch [259/500], lter [354/354], Loss: 0.022172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/500], lter [177/354], Loss: 0.008432\n",
      "Epoch [260/500], lter [354/354], Loss: 0.017943\n",
      "Epoch [261/500], lter [177/354], Loss: 0.047107\n",
      "Epoch [261/500], lter [354/354], Loss: 0.003890\n",
      "Epoch [262/500], lter [177/354], Loss: 0.090581\n",
      "Epoch [262/500], lter [354/354], Loss: 0.069404\n",
      "Epoch [263/500], lter [177/354], Loss: 0.012569\n",
      "Epoch [263/500], lter [354/354], Loss: 0.005013\n",
      "Epoch [264/500], lter [177/354], Loss: 0.042927\n",
      "Epoch [264/500], lter [354/354], Loss: 0.009988\n",
      "Epoch [265/500], lter [177/354], Loss: 0.004792\n",
      "Epoch [265/500], lter [354/354], Loss: 0.097724\n",
      "Epoch [266/500], lter [177/354], Loss: 0.008827\n",
      "Epoch [266/500], lter [354/354], Loss: 0.026646\n",
      "Epoch [267/500], lter [177/354], Loss: 0.043402\n",
      "Epoch [267/500], lter [354/354], Loss: 0.031314\n",
      "Epoch [268/500], lter [177/354], Loss: 0.023740\n",
      "Epoch [268/500], lter [354/354], Loss: 0.012221\n",
      "Epoch [269/500], lter [177/354], Loss: 0.025098\n",
      "Epoch [269/500], lter [354/354], Loss: 0.012924\n",
      "Epoch [270/500], lter [177/354], Loss: 0.002279\n",
      "Epoch [270/500], lter [354/354], Loss: 0.011756\n",
      "Epoch [271/500], lter [177/354], Loss: 0.057705\n",
      "Epoch [271/500], lter [354/354], Loss: 0.079395\n",
      "Epoch [272/500], lter [177/354], Loss: 0.013489\n",
      "Epoch [272/500], lter [354/354], Loss: 0.017615\n",
      "Epoch [273/500], lter [177/354], Loss: 0.037689\n",
      "Epoch [273/500], lter [354/354], Loss: 0.044834\n",
      "Epoch [274/500], lter [177/354], Loss: 0.023949\n",
      "Epoch [274/500], lter [354/354], Loss: 0.032474\n",
      "Epoch [275/500], lter [177/354], Loss: 0.030581\n",
      "Epoch [275/500], lter [354/354], Loss: 0.040437\n",
      "Epoch [276/500], lter [177/354], Loss: 0.030379\n",
      "Epoch [276/500], lter [354/354], Loss: 0.049619\n",
      "Epoch [277/500], lter [177/354], Loss: 0.124755\n",
      "Epoch [277/500], lter [354/354], Loss: 0.011995\n",
      "Epoch [278/500], lter [177/354], Loss: 0.078586\n",
      "Epoch [278/500], lter [354/354], Loss: 0.047305\n",
      "Epoch [279/500], lter [177/354], Loss: 0.039263\n",
      "Epoch [279/500], lter [354/354], Loss: 0.064331\n",
      "Epoch [280/500], lter [177/354], Loss: 0.010303\n",
      "Epoch [280/500], lter [354/354], Loss: 0.041313\n",
      "Epoch [281/500], lter [177/354], Loss: 0.087359\n",
      "Epoch [281/500], lter [354/354], Loss: 0.053082\n",
      "Epoch [282/500], lter [177/354], Loss: 0.027713\n",
      "Epoch [282/500], lter [354/354], Loss: 0.010566\n",
      "Epoch [283/500], lter [177/354], Loss: 0.011031\n",
      "Epoch [283/500], lter [354/354], Loss: 0.053639\n",
      "Epoch [284/500], lter [177/354], Loss: 0.013898\n",
      "Epoch [284/500], lter [354/354], Loss: 0.013067\n",
      "Epoch [285/500], lter [177/354], Loss: 0.016156\n",
      "Epoch [285/500], lter [354/354], Loss: 0.011898\n",
      "Epoch [286/500], lter [177/354], Loss: 0.008308\n",
      "Epoch [286/500], lter [354/354], Loss: 0.080239\n",
      "Epoch [287/500], lter [177/354], Loss: 0.035892\n",
      "Epoch [287/500], lter [354/354], Loss: 0.010343\n",
      "Epoch [288/500], lter [177/354], Loss: 0.010664\n",
      "Epoch [288/500], lter [354/354], Loss: 0.008935\n",
      "Epoch [289/500], lter [177/354], Loss: 0.024622\n",
      "Epoch [289/500], lter [354/354], Loss: 0.007894\n",
      "Epoch [290/500], lter [177/354], Loss: 0.024470\n",
      "Epoch [290/500], lter [354/354], Loss: 0.024948\n",
      "Epoch [291/500], lter [177/354], Loss: 0.019961\n",
      "Epoch [291/500], lter [354/354], Loss: 0.014504\n",
      "Epoch [292/500], lter [177/354], Loss: 0.018147\n",
      "Epoch [292/500], lter [354/354], Loss: 0.014697\n",
      "Epoch [293/500], lter [177/354], Loss: 0.004294\n",
      "Epoch [293/500], lter [354/354], Loss: 0.028688\n",
      "Epoch [294/500], lter [177/354], Loss: 0.014008\n",
      "Epoch [294/500], lter [354/354], Loss: 0.004325\n",
      "Epoch [295/500], lter [177/354], Loss: 0.005219\n",
      "Epoch [295/500], lter [354/354], Loss: 0.047034\n",
      "Epoch [296/500], lter [177/354], Loss: 0.013321\n",
      "Epoch [296/500], lter [354/354], Loss: 0.009348\n",
      "Epoch [297/500], lter [177/354], Loss: 0.006909\n",
      "Epoch [297/500], lter [354/354], Loss: 0.040113\n",
      "Epoch [298/500], lter [177/354], Loss: 0.067995\n",
      "Epoch [298/500], lter [354/354], Loss: 0.095558\n",
      "Epoch [299/500], lter [177/354], Loss: 0.090692\n",
      "Epoch [299/500], lter [354/354], Loss: 0.003095\n",
      "Epoch [300/500], lter [177/354], Loss: 0.006743\n",
      "Epoch [300/500], lter [354/354], Loss: 0.009646\n",
      "Epoch [301/500], lter [177/354], Loss: 0.030497\n",
      "Epoch [301/500], lter [354/354], Loss: 0.025204\n",
      "Epoch [302/500], lter [177/354], Loss: 0.008574\n",
      "Epoch [302/500], lter [354/354], Loss: 0.011216\n",
      "Epoch [303/500], lter [177/354], Loss: 0.060885\n",
      "Epoch [303/500], lter [354/354], Loss: 0.010842\n",
      "Epoch [304/500], lter [177/354], Loss: 0.024605\n",
      "Epoch [304/500], lter [354/354], Loss: 0.013299\n",
      "Epoch [305/500], lter [177/354], Loss: 0.013878\n",
      "Epoch [305/500], lter [354/354], Loss: 0.017417\n",
      "Epoch [306/500], lter [177/354], Loss: 0.008183\n",
      "Epoch [306/500], lter [354/354], Loss: 0.051011\n",
      "Epoch [307/500], lter [177/354], Loss: 0.030004\n",
      "Epoch [307/500], lter [354/354], Loss: 0.035407\n",
      "Epoch [308/500], lter [177/354], Loss: 0.005876\n",
      "Epoch [308/500], lter [354/354], Loss: 0.029161\n",
      "Epoch [309/500], lter [177/354], Loss: 0.016619\n",
      "Epoch [309/500], lter [354/354], Loss: 0.021808\n",
      "Epoch [310/500], lter [177/354], Loss: 0.050311\n",
      "Epoch [310/500], lter [354/354], Loss: 0.063626\n",
      "Epoch [311/500], lter [177/354], Loss: 0.011170\n",
      "Epoch [311/500], lter [354/354], Loss: 0.015401\n",
      "Epoch [312/500], lter [177/354], Loss: 0.038059\n",
      "Epoch [312/500], lter [354/354], Loss: 0.114501\n",
      "Epoch [313/500], lter [177/354], Loss: 0.043768\n",
      "Epoch [313/500], lter [354/354], Loss: 0.048289\n",
      "Epoch [314/500], lter [177/354], Loss: 0.013185\n",
      "Epoch [314/500], lter [354/354], Loss: 0.046463\n",
      "Epoch [315/500], lter [177/354], Loss: 0.052122\n",
      "Epoch [315/500], lter [354/354], Loss: 0.007163\n",
      "Epoch [316/500], lter [177/354], Loss: 0.021282\n",
      "Epoch [316/500], lter [354/354], Loss: 0.030445\n",
      "Epoch [317/500], lter [177/354], Loss: 0.006354\n",
      "Epoch [317/500], lter [354/354], Loss: 0.035425\n",
      "Epoch [318/500], lter [177/354], Loss: 0.021410\n",
      "Epoch [318/500], lter [354/354], Loss: 0.081536\n",
      "Epoch [319/500], lter [177/354], Loss: 0.007149\n",
      "Epoch [319/500], lter [354/354], Loss: 0.034062\n",
      "Epoch [320/500], lter [177/354], Loss: 0.013037\n",
      "Epoch [320/500], lter [354/354], Loss: 0.008489\n",
      "Epoch [321/500], lter [177/354], Loss: 0.050501\n",
      "Epoch [321/500], lter [354/354], Loss: 0.007768\n",
      "Epoch [322/500], lter [177/354], Loss: 0.037134\n",
      "Epoch [322/500], lter [354/354], Loss: 0.023241\n",
      "Epoch [323/500], lter [177/354], Loss: 0.019781\n",
      "Epoch [323/500], lter [354/354], Loss: 0.070573\n",
      "Epoch [324/500], lter [177/354], Loss: 0.028503\n",
      "Epoch [324/500], lter [354/354], Loss: 0.064507\n",
      "Epoch [325/500], lter [177/354], Loss: 0.013469\n",
      "Epoch [325/500], lter [354/354], Loss: 0.010216\n",
      "Epoch [326/500], lter [177/354], Loss: 0.011233\n",
      "Epoch [326/500], lter [354/354], Loss: 0.023654\n",
      "Epoch [327/500], lter [177/354], Loss: 0.081056\n",
      "Epoch [327/500], lter [354/354], Loss: 0.016289\n",
      "Epoch [328/500], lter [177/354], Loss: 0.009861\n",
      "Epoch [328/500], lter [354/354], Loss: 0.031558\n",
      "Epoch [329/500], lter [177/354], Loss: 0.024309\n",
      "Epoch [329/500], lter [354/354], Loss: 0.014871\n",
      "Epoch [330/500], lter [177/354], Loss: 0.014246\n",
      "Epoch [330/500], lter [354/354], Loss: 0.014086\n",
      "Epoch [331/500], lter [177/354], Loss: 0.047582\n",
      "Epoch [331/500], lter [354/354], Loss: 0.017687\n",
      "Epoch [332/500], lter [177/354], Loss: 0.028523\n",
      "Epoch [332/500], lter [354/354], Loss: 0.019644\n",
      "Epoch [333/500], lter [177/354], Loss: 0.012546\n",
      "Epoch [333/500], lter [354/354], Loss: 0.009087\n",
      "Epoch [334/500], lter [177/354], Loss: 0.017314\n",
      "Epoch [334/500], lter [354/354], Loss: 0.031220\n",
      "Epoch [335/500], lter [177/354], Loss: 0.009747\n",
      "Epoch [335/500], lter [354/354], Loss: 0.013493\n",
      "Epoch [336/500], lter [177/354], Loss: 0.010030\n",
      "Epoch [336/500], lter [354/354], Loss: 0.021488\n",
      "Epoch [337/500], lter [177/354], Loss: 0.012743\n",
      "Epoch [337/500], lter [354/354], Loss: 0.008110\n",
      "Epoch [338/500], lter [177/354], Loss: 0.005140\n",
      "Epoch [338/500], lter [354/354], Loss: 0.008903\n",
      "Epoch [339/500], lter [177/354], Loss: 0.018297\n",
      "Epoch [339/500], lter [354/354], Loss: 0.026666\n",
      "Epoch [340/500], lter [177/354], Loss: 0.010224\n",
      "Epoch [340/500], lter [354/354], Loss: 0.102855\n",
      "Epoch [341/500], lter [177/354], Loss: 0.013085\n",
      "Epoch [341/500], lter [354/354], Loss: 0.010140\n",
      "Epoch [342/500], lter [177/354], Loss: 0.057845\n",
      "Epoch [342/500], lter [354/354], Loss: 0.007313\n",
      "Epoch [343/500], lter [177/354], Loss: 0.007155\n",
      "Epoch [343/500], lter [354/354], Loss: 0.050854\n",
      "Epoch [344/500], lter [177/354], Loss: 0.010257\n",
      "Epoch [344/500], lter [354/354], Loss: 0.017519\n",
      "Epoch [345/500], lter [177/354], Loss: 0.024675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [345/500], lter [354/354], Loss: 0.008919\n",
      "Epoch [346/500], lter [177/354], Loss: 0.037060\n",
      "Epoch [346/500], lter [354/354], Loss: 0.015137\n",
      "Epoch [347/500], lter [177/354], Loss: 0.037601\n",
      "Epoch [347/500], lter [354/354], Loss: 0.034827\n",
      "Epoch [348/500], lter [177/354], Loss: 0.008614\n",
      "Epoch [348/500], lter [354/354], Loss: 0.013820\n",
      "Epoch [349/500], lter [177/354], Loss: 0.035439\n",
      "Epoch [349/500], lter [354/354], Loss: 0.019931\n",
      "Epoch [350/500], lter [177/354], Loss: 0.017511\n",
      "Epoch [350/500], lter [354/354], Loss: 0.037929\n",
      "Epoch [351/500], lter [177/354], Loss: 0.003026\n",
      "Epoch [351/500], lter [354/354], Loss: 0.027777\n",
      "Epoch [352/500], lter [177/354], Loss: 0.014957\n",
      "Epoch [352/500], lter [354/354], Loss: 0.010777\n",
      "Epoch [353/500], lter [177/354], Loss: 0.013056\n",
      "Epoch [353/500], lter [354/354], Loss: 0.007225\n",
      "Epoch [354/500], lter [177/354], Loss: 0.008070\n",
      "Epoch [354/500], lter [354/354], Loss: 0.004495\n",
      "Epoch [355/500], lter [177/354], Loss: 0.009195\n",
      "Epoch [355/500], lter [354/354], Loss: 0.005922\n",
      "Epoch [356/500], lter [177/354], Loss: 0.021218\n",
      "Epoch [356/500], lter [354/354], Loss: 0.021503\n",
      "Epoch [357/500], lter [177/354], Loss: 0.018570\n",
      "Epoch [357/500], lter [354/354], Loss: 0.016961\n",
      "Epoch [358/500], lter [177/354], Loss: 0.022142\n",
      "Epoch [358/500], lter [354/354], Loss: 0.053819\n",
      "Epoch [359/500], lter [177/354], Loss: 0.017704\n",
      "Epoch [359/500], lter [354/354], Loss: 0.013296\n",
      "Epoch [360/500], lter [177/354], Loss: 0.024427\n",
      "Epoch [360/500], lter [354/354], Loss: 0.013348\n",
      "Epoch [361/500], lter [177/354], Loss: 0.011430\n",
      "Epoch [361/500], lter [354/354], Loss: 0.069636\n",
      "Epoch [362/500], lter [177/354], Loss: 0.005671\n",
      "Epoch [362/500], lter [354/354], Loss: 0.064808\n",
      "Epoch [363/500], lter [177/354], Loss: 0.069880\n",
      "Epoch [363/500], lter [354/354], Loss: 0.011926\n",
      "Epoch [364/500], lter [177/354], Loss: 0.034134\n",
      "Epoch [364/500], lter [354/354], Loss: 0.011349\n",
      "Epoch [365/500], lter [177/354], Loss: 0.007098\n",
      "Epoch [365/500], lter [354/354], Loss: 0.031276\n",
      "Epoch [366/500], lter [177/354], Loss: 0.013936\n",
      "Epoch [366/500], lter [354/354], Loss: 0.003048\n",
      "Epoch [367/500], lter [177/354], Loss: 0.006300\n",
      "Epoch [367/500], lter [354/354], Loss: 0.013710\n",
      "Epoch [368/500], lter [177/354], Loss: 0.012447\n",
      "Epoch [368/500], lter [354/354], Loss: 0.006573\n",
      "Epoch [369/500], lter [177/354], Loss: 0.045371\n",
      "Epoch [369/500], lter [354/354], Loss: 0.021812\n",
      "Epoch [370/500], lter [177/354], Loss: 0.012970\n",
      "Epoch [370/500], lter [354/354], Loss: 0.007679\n",
      "Epoch [371/500], lter [177/354], Loss: 0.013863\n",
      "Epoch [371/500], lter [354/354], Loss: 0.030486\n",
      "Epoch [372/500], lter [177/354], Loss: 0.019910\n",
      "Epoch [372/500], lter [354/354], Loss: 0.032186\n",
      "Epoch [373/500], lter [177/354], Loss: 0.004268\n",
      "Epoch [373/500], lter [354/354], Loss: 0.025647\n",
      "Epoch [374/500], lter [177/354], Loss: 0.079160\n",
      "Epoch [374/500], lter [354/354], Loss: 0.127213\n",
      "Epoch [375/500], lter [177/354], Loss: 0.008112\n",
      "Epoch [375/500], lter [354/354], Loss: 0.007470\n",
      "Epoch [376/500], lter [177/354], Loss: 0.016752\n",
      "Epoch [376/500], lter [354/354], Loss: 0.009006\n",
      "Epoch [377/500], lter [177/354], Loss: 0.029683\n",
      "Epoch [377/500], lter [354/354], Loss: 0.002529\n",
      "Epoch [378/500], lter [177/354], Loss: 0.082301\n",
      "Epoch [378/500], lter [354/354], Loss: 0.006979\n",
      "Epoch [379/500], lter [177/354], Loss: 0.015633\n",
      "Epoch [379/500], lter [354/354], Loss: 0.011008\n",
      "Epoch [380/500], lter [177/354], Loss: 0.037584\n",
      "Epoch [380/500], lter [354/354], Loss: 0.097084\n",
      "Epoch [381/500], lter [177/354], Loss: 0.042397\n",
      "Epoch [381/500], lter [354/354], Loss: 0.009526\n",
      "Epoch [382/500], lter [177/354], Loss: 0.015216\n",
      "Epoch [382/500], lter [354/354], Loss: 0.007302\n",
      "Epoch [383/500], lter [177/354], Loss: 0.008152\n",
      "Epoch [383/500], lter [354/354], Loss: 0.019220\n",
      "Epoch [384/500], lter [177/354], Loss: 0.014019\n",
      "Epoch [384/500], lter [354/354], Loss: 0.020851\n",
      "Epoch [385/500], lter [177/354], Loss: 0.019278\n",
      "Epoch [385/500], lter [354/354], Loss: 0.073902\n",
      "Epoch [386/500], lter [177/354], Loss: 0.010829\n",
      "Epoch [386/500], lter [354/354], Loss: 0.010780\n",
      "Epoch [387/500], lter [177/354], Loss: 0.023500\n",
      "Epoch [387/500], lter [354/354], Loss: 0.013971\n",
      "Epoch [388/500], lter [177/354], Loss: 0.018434\n",
      "Epoch [388/500], lter [354/354], Loss: 0.006183\n",
      "Epoch [389/500], lter [177/354], Loss: 0.016970\n",
      "Epoch [389/500], lter [354/354], Loss: 0.014010\n",
      "Epoch [390/500], lter [177/354], Loss: 0.006008\n",
      "Epoch [390/500], lter [354/354], Loss: 0.006006\n",
      "Epoch [391/500], lter [177/354], Loss: 0.008953\n",
      "Epoch [391/500], lter [354/354], Loss: 0.023473\n",
      "Epoch [392/500], lter [177/354], Loss: 0.013007\n",
      "Epoch [392/500], lter [354/354], Loss: 0.016413\n",
      "Epoch [393/500], lter [177/354], Loss: 0.027408\n",
      "Epoch [393/500], lter [354/354], Loss: 0.106045\n",
      "Epoch [394/500], lter [177/354], Loss: 0.027752\n",
      "Epoch [394/500], lter [354/354], Loss: 0.032629\n",
      "Epoch [395/500], lter [177/354], Loss: 0.031518\n",
      "Epoch [395/500], lter [354/354], Loss: 0.101441\n",
      "Epoch [396/500], lter [177/354], Loss: 0.025522\n",
      "Epoch [396/500], lter [354/354], Loss: 0.008995\n",
      "Epoch [397/500], lter [177/354], Loss: 0.011003\n",
      "Epoch [397/500], lter [354/354], Loss: 0.019158\n",
      "Epoch [398/500], lter [177/354], Loss: 0.065506\n",
      "Epoch [398/500], lter [354/354], Loss: 0.012028\n",
      "Epoch [399/500], lter [177/354], Loss: 0.034573\n",
      "Epoch [399/500], lter [354/354], Loss: 0.053627\n",
      "Epoch [400/500], lter [177/354], Loss: 0.020319\n",
      "Epoch [400/500], lter [354/354], Loss: 0.026037\n",
      "Epoch [401/500], lter [177/354], Loss: 0.020798\n",
      "Epoch [401/500], lter [354/354], Loss: 0.022007\n",
      "Epoch [402/500], lter [177/354], Loss: 0.012382\n",
      "Epoch [402/500], lter [354/354], Loss: 0.008665\n",
      "Epoch [403/500], lter [177/354], Loss: 0.007728\n",
      "Epoch [403/500], lter [354/354], Loss: 0.004213\n",
      "Epoch [404/500], lter [177/354], Loss: 0.004081\n",
      "Epoch [404/500], lter [354/354], Loss: 0.014683\n",
      "Epoch [405/500], lter [177/354], Loss: 0.017644\n",
      "Epoch [405/500], lter [354/354], Loss: 0.036329\n",
      "Epoch [406/500], lter [177/354], Loss: 0.007220\n",
      "Epoch [406/500], lter [354/354], Loss: 0.013270\n",
      "Epoch [407/500], lter [177/354], Loss: 0.009772\n",
      "Epoch [407/500], lter [354/354], Loss: 0.009875\n",
      "Epoch [408/500], lter [177/354], Loss: 0.030897\n",
      "Epoch [408/500], lter [354/354], Loss: 0.092918\n",
      "Epoch [409/500], lter [177/354], Loss: 0.011425\n",
      "Epoch [409/500], lter [354/354], Loss: 0.003005\n",
      "Epoch [410/500], lter [177/354], Loss: 0.066917\n",
      "Epoch [410/500], lter [354/354], Loss: 0.002611\n",
      "Epoch [411/500], lter [177/354], Loss: 0.027961\n",
      "Epoch [411/500], lter [354/354], Loss: 0.022186\n",
      "Epoch [412/500], lter [177/354], Loss: 0.008684\n",
      "Epoch [412/500], lter [354/354], Loss: 0.009673\n",
      "Epoch [413/500], lter [177/354], Loss: 0.005935\n",
      "Epoch [413/500], lter [354/354], Loss: 0.015258\n",
      "Epoch [414/500], lter [177/354], Loss: 0.070860\n",
      "Epoch [414/500], lter [354/354], Loss: 0.025447\n",
      "Epoch [415/500], lter [177/354], Loss: 0.057283\n",
      "Epoch [415/500], lter [354/354], Loss: 0.009887\n",
      "Epoch [416/500], lter [177/354], Loss: 0.015543\n",
      "Epoch [416/500], lter [354/354], Loss: 0.057405\n",
      "Epoch [417/500], lter [177/354], Loss: 0.038013\n",
      "Epoch [417/500], lter [354/354], Loss: 0.014075\n",
      "Epoch [418/500], lter [177/354], Loss: 0.010063\n",
      "Epoch [418/500], lter [354/354], Loss: 0.007025\n",
      "Epoch [419/500], lter [177/354], Loss: 0.015453\n",
      "Epoch [419/500], lter [354/354], Loss: 0.018412\n",
      "Epoch [420/500], lter [177/354], Loss: 0.023394\n",
      "Epoch [420/500], lter [354/354], Loss: 0.016780\n",
      "Epoch [421/500], lter [177/354], Loss: 0.035251\n",
      "Epoch [421/500], lter [354/354], Loss: 0.009633\n",
      "Epoch [422/500], lter [177/354], Loss: 0.012645\n",
      "Epoch [422/500], lter [354/354], Loss: 0.017928\n",
      "Epoch [423/500], lter [177/354], Loss: 0.008287\n",
      "Epoch [423/500], lter [354/354], Loss: 0.035922\n",
      "Epoch [424/500], lter [177/354], Loss: 0.016539\n",
      "Epoch [424/500], lter [354/354], Loss: 0.014654\n",
      "Epoch [425/500], lter [177/354], Loss: 0.038647\n",
      "Epoch [425/500], lter [354/354], Loss: 0.026315\n",
      "Epoch [426/500], lter [177/354], Loss: 0.022802\n",
      "Epoch [426/500], lter [354/354], Loss: 0.042195\n",
      "Epoch [427/500], lter [177/354], Loss: 0.004708\n",
      "Epoch [427/500], lter [354/354], Loss: 0.030574\n",
      "Epoch [428/500], lter [177/354], Loss: 0.109638\n",
      "Epoch [428/500], lter [354/354], Loss: 0.045726\n",
      "Epoch [429/500], lter [177/354], Loss: 0.013469\n",
      "Epoch [429/500], lter [354/354], Loss: 0.055486\n",
      "Epoch [430/500], lter [177/354], Loss: 0.018641\n",
      "Epoch [430/500], lter [354/354], Loss: 0.034321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [431/500], lter [177/354], Loss: 0.017543\n",
      "Epoch [431/500], lter [354/354], Loss: 0.059729\n",
      "Epoch [432/500], lter [177/354], Loss: 0.026644\n",
      "Epoch [432/500], lter [354/354], Loss: 0.009100\n",
      "Epoch [433/500], lter [177/354], Loss: 0.021734\n",
      "Epoch [433/500], lter [354/354], Loss: 0.025194\n",
      "Epoch [434/500], lter [177/354], Loss: 0.027408\n",
      "Epoch [434/500], lter [354/354], Loss: 0.058511\n",
      "Epoch [435/500], lter [177/354], Loss: 0.022110\n",
      "Epoch [435/500], lter [354/354], Loss: 0.039793\n",
      "Epoch [436/500], lter [177/354], Loss: 0.019929\n",
      "Epoch [436/500], lter [354/354], Loss: 0.051107\n",
      "Epoch [437/500], lter [177/354], Loss: 0.018474\n",
      "Epoch [437/500], lter [354/354], Loss: 0.027441\n",
      "Epoch [438/500], lter [177/354], Loss: 0.027411\n",
      "Epoch [438/500], lter [354/354], Loss: 0.045378\n",
      "Epoch [439/500], lter [177/354], Loss: 0.026342\n",
      "Epoch [439/500], lter [354/354], Loss: 0.056116\n",
      "Epoch [440/500], lter [177/354], Loss: 0.010557\n",
      "Epoch [440/500], lter [354/354], Loss: 0.015070\n",
      "Epoch [441/500], lter [177/354], Loss: 0.044709\n",
      "Epoch [441/500], lter [354/354], Loss: 0.012177\n",
      "Epoch [442/500], lter [177/354], Loss: 0.032722\n",
      "Epoch [442/500], lter [354/354], Loss: 0.028447\n",
      "Epoch [443/500], lter [177/354], Loss: 0.007365\n",
      "Epoch [443/500], lter [354/354], Loss: 0.020746\n",
      "Epoch [444/500], lter [177/354], Loss: 0.060751\n",
      "Epoch [444/500], lter [354/354], Loss: 0.028454\n",
      "Epoch [445/500], lter [177/354], Loss: 0.033562\n",
      "Epoch [445/500], lter [354/354], Loss: 0.018974\n",
      "Epoch [446/500], lter [177/354], Loss: 0.006740\n",
      "Epoch [446/500], lter [354/354], Loss: 0.017794\n",
      "Epoch [447/500], lter [177/354], Loss: 0.012317\n",
      "Epoch [447/500], lter [354/354], Loss: 0.052009\n",
      "Epoch [448/500], lter [177/354], Loss: 0.011125\n",
      "Epoch [448/500], lter [354/354], Loss: 0.025241\n",
      "Epoch [449/500], lter [177/354], Loss: 0.023351\n",
      "Epoch [449/500], lter [354/354], Loss: 0.011231\n",
      "Epoch [450/500], lter [177/354], Loss: 0.020002\n",
      "Epoch [450/500], lter [354/354], Loss: 0.018617\n",
      "Epoch [451/500], lter [177/354], Loss: 0.013106\n",
      "Epoch [451/500], lter [354/354], Loss: 0.008490\n",
      "Epoch [452/500], lter [177/354], Loss: 0.025426\n",
      "Epoch [452/500], lter [354/354], Loss: 0.044295\n",
      "Epoch [453/500], lter [177/354], Loss: 0.002167\n",
      "Epoch [453/500], lter [354/354], Loss: 0.007630\n",
      "Epoch [454/500], lter [177/354], Loss: 0.020248\n",
      "Epoch [454/500], lter [354/354], Loss: 0.024430\n",
      "Epoch [455/500], lter [177/354], Loss: 0.019375\n",
      "Epoch [455/500], lter [354/354], Loss: 0.013322\n",
      "Epoch [456/500], lter [177/354], Loss: 0.015981\n",
      "Epoch [456/500], lter [354/354], Loss: 0.062528\n",
      "Epoch [457/500], lter [177/354], Loss: 0.021631\n",
      "Epoch [457/500], lter [354/354], Loss: 0.008822\n",
      "Epoch [458/500], lter [177/354], Loss: 0.013990\n",
      "Epoch [458/500], lter [354/354], Loss: 0.004737\n",
      "Epoch [459/500], lter [177/354], Loss: 0.034634\n",
      "Epoch [459/500], lter [354/354], Loss: 0.020866\n",
      "Epoch [460/500], lter [177/354], Loss: 0.014454\n",
      "Epoch [460/500], lter [354/354], Loss: 0.011520\n",
      "Epoch [461/500], lter [177/354], Loss: 0.024932\n",
      "Epoch [461/500], lter [354/354], Loss: 0.019756\n",
      "Epoch [462/500], lter [177/354], Loss: 0.036982\n",
      "Epoch [462/500], lter [354/354], Loss: 0.010046\n",
      "Epoch [463/500], lter [177/354], Loss: 0.012108\n",
      "Epoch [463/500], lter [354/354], Loss: 0.023479\n",
      "Epoch [464/500], lter [177/354], Loss: 0.014199\n",
      "Epoch [464/500], lter [354/354], Loss: 0.025936\n",
      "Epoch [465/500], lter [177/354], Loss: 0.023622\n",
      "Epoch [465/500], lter [354/354], Loss: 0.016199\n",
      "Epoch [466/500], lter [177/354], Loss: 0.023447\n",
      "Epoch [466/500], lter [354/354], Loss: 0.039359\n",
      "Epoch [467/500], lter [177/354], Loss: 0.021137\n",
      "Epoch [467/500], lter [354/354], Loss: 0.046217\n",
      "Epoch [468/500], lter [177/354], Loss: 0.046693\n",
      "Epoch [468/500], lter [354/354], Loss: 0.007349\n",
      "Epoch [469/500], lter [177/354], Loss: 0.003532\n",
      "Epoch [469/500], lter [354/354], Loss: 0.011663\n",
      "Epoch [470/500], lter [177/354], Loss: 0.073760\n",
      "Epoch [470/500], lter [354/354], Loss: 0.034713\n",
      "Epoch [471/500], lter [177/354], Loss: 0.071158\n",
      "Epoch [471/500], lter [354/354], Loss: 0.014310\n",
      "Epoch [472/500], lter [177/354], Loss: 0.011714\n",
      "Epoch [472/500], lter [354/354], Loss: 0.031443\n",
      "Epoch [473/500], lter [177/354], Loss: 0.050208\n",
      "Epoch [473/500], lter [354/354], Loss: 0.009590\n",
      "Epoch [474/500], lter [177/354], Loss: 0.013026\n",
      "Epoch [474/500], lter [354/354], Loss: 0.007285\n",
      "Epoch [475/500], lter [177/354], Loss: 0.032040\n",
      "Epoch [475/500], lter [354/354], Loss: 0.015059\n",
      "Epoch [476/500], lter [177/354], Loss: 0.005607\n",
      "Epoch [476/500], lter [354/354], Loss: 0.010204\n",
      "Epoch [477/500], lter [177/354], Loss: 0.016045\n",
      "Epoch [477/500], lter [354/354], Loss: 0.069969\n",
      "Epoch [478/500], lter [177/354], Loss: 0.013659\n",
      "Epoch [478/500], lter [354/354], Loss: 0.005342\n",
      "Epoch [479/500], lter [177/354], Loss: 0.021491\n",
      "Epoch [479/500], lter [354/354], Loss: 0.028903\n",
      "Epoch [480/500], lter [177/354], Loss: 0.048945\n",
      "Epoch [480/500], lter [354/354], Loss: 0.082906\n",
      "Epoch [481/500], lter [177/354], Loss: 0.062078\n",
      "Epoch [481/500], lter [354/354], Loss: 0.014365\n",
      "Epoch [482/500], lter [177/354], Loss: 0.015928\n",
      "Epoch [482/500], lter [354/354], Loss: 0.009433\n",
      "Epoch [483/500], lter [177/354], Loss: 0.160907\n",
      "Epoch [483/500], lter [354/354], Loss: 0.004703\n",
      "Epoch [484/500], lter [177/354], Loss: 0.010692\n",
      "Epoch [484/500], lter [354/354], Loss: 0.010943\n",
      "Epoch [485/500], lter [177/354], Loss: 0.009635\n",
      "Epoch [485/500], lter [354/354], Loss: 0.017776\n",
      "Epoch [486/500], lter [177/354], Loss: 0.049371\n",
      "Epoch [486/500], lter [354/354], Loss: 0.041850\n",
      "Epoch [487/500], lter [177/354], Loss: 0.044607\n",
      "Epoch [487/500], lter [354/354], Loss: 0.011507\n",
      "Epoch [488/500], lter [177/354], Loss: 0.018089\n",
      "Epoch [488/500], lter [354/354], Loss: 0.014135\n",
      "Epoch [489/500], lter [177/354], Loss: 0.019181\n",
      "Epoch [489/500], lter [354/354], Loss: 0.033427\n",
      "Epoch [490/500], lter [177/354], Loss: 0.010455\n",
      "Epoch [490/500], lter [354/354], Loss: 0.014061\n",
      "Epoch [491/500], lter [177/354], Loss: 0.026209\n",
      "Epoch [491/500], lter [354/354], Loss: 0.012639\n",
      "Epoch [492/500], lter [177/354], Loss: 0.020194\n",
      "Epoch [492/500], lter [354/354], Loss: 0.010414\n",
      "Epoch [493/500], lter [177/354], Loss: 0.008043\n",
      "Epoch [493/500], lter [354/354], Loss: 0.004365\n",
      "Epoch [494/500], lter [177/354], Loss: 0.003380\n",
      "Epoch [494/500], lter [354/354], Loss: 0.014741\n",
      "Epoch [495/500], lter [177/354], Loss: 0.010784\n",
      "Epoch [495/500], lter [354/354], Loss: 0.003945\n",
      "Epoch [496/500], lter [177/354], Loss: 0.008521\n",
      "Epoch [496/500], lter [354/354], Loss: 0.016979\n",
      "Epoch [497/500], lter [177/354], Loss: 0.008184\n",
      "Epoch [497/500], lter [354/354], Loss: 0.013579\n",
      "Epoch [498/500], lter [177/354], Loss: 0.015790\n",
      "Epoch [498/500], lter [354/354], Loss: 0.012118\n",
      "Epoch [499/500], lter [177/354], Loss: 0.006100\n",
      "Epoch [499/500], lter [354/354], Loss: 0.026291\n",
      "Epoch [500/500], lter [177/354], Loss: 0.009090\n",
      "Epoch [500/500], lter [354/354], Loss: 0.012151\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "cost_list = []\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_batch = len(train_data) // batch_size\n",
    "    \n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        batch_data = batch_data.to(device)\n",
    "        \n",
    "        reconst_data = model(batch_data)\n",
    "        cost = loss(reconst_data, batch_data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        if (i+1) % (total_batch//2) == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d], Loss: %.6f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "            \n",
    "        # early stopping rule 1 : MSE < 1e-06\n",
    "        if cost.item() < 1e-06 :\n",
    "            early_stop = True\n",
    "            break\n",
    "            \n",
    "#         early stopping rule 2 : simple moving average of length 5\n",
    "#         sometimes it doesn't work well.\n",
    "#         if len(cost_list) > 5 :\n",
    "#            if cost.item() > np.mean(cost_list[-5:]):\n",
    "#                early_stop = True\n",
    "#                break\n",
    "                \n",
    "        cost_list.append(cost.item())\n",
    "\n",
    "    if early_stop :\n",
    "        break\n",
    "        \n",
    "print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE_SUM : 3.1379940845893466\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "filled_data = model(missed_data.to(device))\n",
    "filled_data = filled_data.cpu().detach().numpy()\n",
    "\n",
    "rmse_sum = 0\n",
    "\n",
    "for i in range(cols) :\n",
    "    if mask[:,i].sum() > 0 :\n",
    "        y_actual = test_data[:,i][mask[:,i]]\n",
    "        y_predicted = filled_data[:,i][mask[:,i]]\n",
    "\n",
    "        rmse = sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "        rmse_sum += rmse\n",
    "    \n",
    "print(\"RMSE_SUM :\", rmse_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
